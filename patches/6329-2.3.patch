diff --git a/clients/src/main/java/org/apache/kafka/common/record/FileLogInputStream.java b/clients/src/main/java/org/apache/kafka/common/record/FileLogInputStream.java
index 15c09dea3..cda19a1e8 100644
--- a/clients/src/main/java/org/apache/kafka/common/record/FileLogInputStream.java
+++ b/clients/src/main/java/org/apache/kafka/common/record/FileLogInputStream.java
@@ -228,21 +228,16 @@ public class FileLogInputStream implements LogInputStream<FileLogInputStream.Fil
 
             FileChannelRecordBatch that = (FileChannelRecordBatch) o;
 
-            FileChannel channel = fileRecords == null ? null : fileRecords.channel();
-            FileChannel thatChannel = that.fileRecords == null ? null : that.fileRecords.channel();
-
             return offset == that.offset &&
                     position == that.position &&
                     batchSize == that.batchSize &&
-                    Objects.equals(channel, thatChannel);
+                    Objects.equals(fileRecords, that.fileRecords);
         }
 
         @Override
         public int hashCode() {
-            FileChannel channel = fileRecords == null ? null : fileRecords.channel();
-
             int result = Long.hashCode(offset);
-            result = 31 * result + (channel != null ? channel.hashCode() : 0);
+            result = 31 * result + (fileRecords != null ? fileRecords.hashCode() : 0);
             result = 31 * result + position;
             result = 31 * result + batchSize;
             return result;
diff --git a/clients/src/main/java/org/apache/kafka/common/record/FileRecords.java b/clients/src/main/java/org/apache/kafka/common/record/FileRecords.java
index a02fb4135..6b0c23ced 100644
--- a/clients/src/main/java/org/apache/kafka/common/record/FileRecords.java
+++ b/clients/src/main/java/org/apache/kafka/common/record/FileRecords.java
@@ -22,11 +22,13 @@ import org.apache.kafka.common.record.FileLogInputStream.FileChannelRecordBatch;
 import org.apache.kafka.common.utils.AbstractIterator;
 import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.common.utils.Utils;
+import org.apache.kafka.common.utils.OperatingSystem;
 
 import java.io.Closeable;
 import java.io.File;
 import java.io.IOException;
 import java.io.RandomAccessFile;
+import java.io.UncheckedIOException;
 import java.nio.ByteBuffer;
 import java.nio.channels.FileChannel;
 import java.nio.channels.GatheringByteChannel;
@@ -41,6 +43,7 @@ import java.util.concurrent.atomic.AtomicInteger;
  * instance to enable slicing a range of the log records.
  */
 public class FileRecords extends AbstractRecords implements Closeable {
+    private final Object mutex = new Object();
     private final boolean isSlice;
     private final int start;
     private final int end;
@@ -49,8 +52,8 @@ public class FileRecords extends AbstractRecords implements Closeable {
 
     // mutable state
     private final AtomicInteger size;
-    private final FileChannel channel;
     private volatile File file;
+    private volatile FileChannel channel;
 
     /**
      * The {@code FileRecords.open} methods should be used instead of this constructor whenever possible.
@@ -105,7 +108,21 @@ public class FileRecords extends AbstractRecords implements Closeable {
      * @return The file channel
      */
     public FileChannel channel() {
-        return channel;
+        if (OperatingSystem.IS_WINDOWS) {
+            synchronized (mutex) {
+                if (channel == null) {
+                    try {
+                        channel = FileChannel.open(file.toPath(), StandardOpenOption.CREATE, StandardOpenOption.READ,
+                                StandardOpenOption.WRITE);
+                    } catch (IOException e) {
+                        throw new UncheckedIOException(e);
+                    }
+                }
+                return channel;
+            }
+        } else {
+            return channel;
+        }
     }
 
     /**
@@ -118,7 +135,7 @@ public class FileRecords extends AbstractRecords implements Closeable {
      * possible exceptions
      */
     public void readInto(ByteBuffer buffer, int position) throws IOException {
-        Utils.readFully(channel, buffer, position + this.start);
+        Utils.readFully(channel(), buffer, position + this.start);
         buffer.flip();
     }
 
@@ -146,7 +163,7 @@ public class FileRecords extends AbstractRecords implements Closeable {
         // handle integer overflow or if end is beyond the end of the file
         if (end < 0 || end >= start + sizeInBytes())
             end = start + sizeInBytes();
-        return new FileRecords(file, channel, this.start + position, end, true);
+        return new FileRecords(file, channel(), this.start + position, end, true);
     }
 
     /**
@@ -161,7 +178,7 @@ public class FileRecords extends AbstractRecords implements Closeable {
             throw new IllegalArgumentException("Append of size " + records.sizeInBytes() +
                     " bytes is too large for segment with current file position at " + size.get());
 
-        int written = records.writeFullyTo(channel);
+        int written = records.writeFullyTo(channel());
         size.getAndAdd(written);
         return written;
     }
@@ -170,7 +187,9 @@ public class FileRecords extends AbstractRecords implements Closeable {
      * Commit all written data to the physical disk
      */
     public void flush() throws IOException {
-        channel.force(true);
+        if (channel != null) {
+            channel.force(true);
+        }
     }
 
     /**
@@ -179,14 +198,18 @@ public class FileRecords extends AbstractRecords implements Closeable {
     public void close() throws IOException {
         flush();
         trim();
-        channel.close();
+        if (channel != null) {
+            channel.close();
+        }
     }
 
     /**
      * Close file handlers used by the FileChannel but don't write to disk. This is used when the disk may have failed
      */
     public void closeHandlers() throws IOException {
-        channel.close();
+        if (channel != null) {
+            channel.close();
+        }
     }
 
     /**
@@ -196,8 +219,19 @@ public class FileRecords extends AbstractRecords implements Closeable {
      *          because it did not exist
      */
     public boolean deleteIfExists() throws IOException {
-        Utils.closeQuietly(channel, "FileChannel");
-        return Files.deleteIfExists(file.toPath());
+        if (OperatingSystem.IS_WINDOWS) {
+            synchronized (mutex) {
+                if (channel != null) {
+                    Utils.closeQuietly(channel, "FileChannel");
+                }
+                return Files.deleteIfExists(file.toPath());
+            }
+        } else {
+            if (channel != null) {
+                Utils.closeQuietly(channel, "FileChannel");
+            }
+            return Files.deleteIfExists(file.toPath());
+        }
     }
 
     /**
@@ -220,10 +254,25 @@ public class FileRecords extends AbstractRecords implements Closeable {
      * @throws IOException if rename fails.
      */
     public void renameTo(File f) throws IOException {
-        try {
-            Utils.atomicMoveWithFallback(file.toPath(), f.toPath());
-        } finally {
-            this.file = f;
+        if (OperatingSystem.IS_WINDOWS) {
+            synchronized (mutex) {
+                try {
+                    close();
+                    try {
+                        Utils.atomicMoveWithFallback(file.toPath(), f.toPath());
+                    } finally {
+                        this.file = f;
+                    }
+                } finally {
+                    channel = null;
+                }
+            }
+        } else {
+            try {
+                Utils.atomicMoveWithFallback(file.toPath(), f.toPath());
+            } finally {
+                this.file = f;
+            }
         }
     }
 
@@ -242,8 +291,8 @@ public class FileRecords extends AbstractRecords implements Closeable {
         if (targetSize > originalSize || targetSize < 0)
             throw new KafkaException("Attempt to truncate log segment " + file + " to " + targetSize + " bytes failed, " +
                     " size of this log segment is " + originalSize + " bytes.");
-        if (targetSize < (int) channel.size()) {
-            channel.truncate(targetSize);
+        if (targetSize < (int) channel().size()) {
+            channel().truncate(targetSize);
             size.set(targetSize);
         }
         return originalSize - targetSize;
@@ -268,7 +317,7 @@ public class FileRecords extends AbstractRecords implements Closeable {
 
     @Override
     public long writeTo(GatheringByteChannel destChannel, long offset, int length) throws IOException {
-        long newSize = Math.min(channel.size(), end) - start;
+        long newSize = Math.min(channel().size(), end) - start;
         int oldSize = sizeInBytes();
         if (newSize < oldSize)
             throw new KafkaException(String.format(
@@ -280,9 +329,9 @@ public class FileRecords extends AbstractRecords implements Closeable {
         final long bytesTransferred;
         if (destChannel instanceof TransportLayer) {
             TransportLayer tl = (TransportLayer) destChannel;
-            bytesTransferred = tl.transferFrom(channel, position, count);
+            bytesTransferred = tl.transferFrom(channel(), position, count);
         } else {
-            bytesTransferred = channel.transferTo(position, count, destChannel);
+            bytesTransferred = channel().transferTo(position, count, destChannel);
         }
         return bytesTransferred;
     }
@@ -456,6 +505,23 @@ public class FileRecords extends AbstractRecords implements Closeable {
         }
     }
 
+    @Override
+    public int hashCode() {
+        return file.hashCode();
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o)
+            return true;
+        if (o == null || getClass() != o.getClass())
+            return false;
+
+        FileRecords that = (FileRecords) o;
+
+        return file.equals(that.file);
+    }
+
     public static class LogOffsetPosition {
         public final long offset;
         public final int position;
diff --git a/core/src/main/scala/kafka/log/AbstractIndex.scala b/core/src/main/scala/kafka/log/AbstractIndex.scala
index 05237c441..ce3c098de 100644
--- a/core/src/main/scala/kafka/log/AbstractIndex.scala
+++ b/core/src/main/scala/kafka/log/AbstractIndex.scala
@@ -108,47 +108,60 @@ abstract class AbstractIndex[K, V](@volatile var file: File, val baseOffset: Lon
 
   protected val lock = new ReentrantLock
 
-  @volatile
-  protected var mmap: MappedByteBuffer = {
-    val newlyCreated = file.createNewFile()
-    val raf = if (writable) new RandomAccessFile(file, "rw") else new RandomAccessFile(file, "r")
-    try {
-      /* pre-allocate the file if necessary */
-      if(newlyCreated) {
-        if(maxIndexSize < entrySize)
-          throw new IllegalArgumentException("Invalid max index size: " + maxIndexSize)
-        raf.setLength(roundDownToExactMultiple(maxIndexSize, entrySize))
-      }
-
-      /* memory-map the file */
-      _length = raf.length()
-      val idx = {
-        if (writable)
-          raf.getChannel.map(FileChannel.MapMode.READ_WRITE, 0, _length)
-        else
-          raf.getChannel.map(FileChannel.MapMode.READ_ONLY, 0, _length)
-      }
-      /* set the position in the index for the next entry */
-      if(newlyCreated)
-        idx.position(0)
-      else
-        // if this is a pre-existing index, assume it is valid and set position to last entry
-        idx.position(roundDownToExactMultiple(idx.limit(), entrySize))
-      idx
-    } finally {
-      CoreUtils.swallow(raf.close(), AbstractIndex)
-    }
-  }
-
   /**
    * The maximum number of entries this index can hold
    */
   @volatile
-  private[this] var _maxEntries = mmap.limit() / entrySize
+  private[this] var _maxEntries: Int = _
 
   /** The number of entries in this index */
   @volatile
-  protected var _entries = mmap.position() / entrySize
+  protected var _entries: Int = _
+
+  @volatile
+  protected var _mmap: MappedByteBuffer = _
+
+  protected def mmap(): MappedByteBuffer = {
+    maybeLock(lock) {
+      if (_mmap == null) {
+        val newlyCreated = file.createNewFile()
+        val raf = if (writable) new RandomAccessFile(file, "rw") else new RandomAccessFile(file, "r")
+        try {
+          /* pre-allocate the file if necessary */
+          if(newlyCreated) {
+            if(maxIndexSize < entrySize)
+              throw new IllegalArgumentException("Invalid max index size: " + maxIndexSize)
+            raf.setLength(roundDownToExactMultiple(maxIndexSize, entrySize))
+          }
+
+          /* memory-map the file */
+          _length = raf.length();
+          val idx = {
+            if (writable)
+              raf.getChannel.map(FileChannel.MapMode.READ_WRITE, 0, _length)
+            else
+              raf.getChannel.map(FileChannel.MapMode.READ_ONLY, 0, _length)
+          }
+          /* set the position in the index for the next entry */
+          if(newlyCreated)
+            idx.position(0)
+          else
+            // if this is a pre-existing index assume it is valid and set position to last entry
+            idx.position(roundDownToExactMultiple(idx.limit(), entrySize))
+
+          // Set resulting mmap to instance variables
+          _mmap = idx
+          _maxEntries = _mmap.limit() / entrySize
+          _entries = _mmap.position() / entrySize
+        } finally {
+          CoreUtils.swallow(raf.close(), AbstractIndex)
+        }
+      }
+      return _mmap
+    }
+  }
+
+  mmap()
 
   /**
    * True iff there are no more slots available in this index
@@ -187,11 +200,11 @@ abstract class AbstractIndex[K, V](@volatile var file: File, val baseOffset: Lon
             safeForceUnmap()
           raf.setLength(roundedNewSize)
           _length = roundedNewSize
-          mmap = raf.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, roundedNewSize)
+          _mmap = raf.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, roundedNewSize)
           _maxEntries = mmap.limit() / entrySize
-          mmap.position(position)
-          debug(s"Resized ${file.getAbsolutePath} to $roundedNewSize, position is ${mmap.position()} " +
-            s"and limit is ${mmap.limit()}")
+          _mmap.position(position)
+          debug(s"Resized ${file.getAbsolutePath} to $roundedNewSize, position is ${_mmap.position()} " +
+            s"and limit is ${_mmap.limit()}")
           true
         } finally {
           CoreUtils.swallow(raf.close(), AbstractIndex)
@@ -206,8 +219,10 @@ abstract class AbstractIndex[K, V](@volatile var file: File, val baseOffset: Lon
    * @throws IOException if rename fails
    */
   def renameTo(f: File) {
-    try Utils.atomicMoveWithFallback(file.toPath, f.toPath)
-    finally file = f
+    maybeUnmap(lock) {
+      try Utils.atomicMoveWithFallback(file.toPath, f.toPath)
+      finally file = f
+    }
   }
 
   /**
@@ -227,8 +242,10 @@ abstract class AbstractIndex[K, V](@volatile var file: File, val baseOffset: Lon
    *         not exist
    */
   def deleteIfExists(): Boolean = {
-    closeHandler()
-    Files.deleteIfExists(file.toPath)
+    maybeLock(lock) {
+      closeHandler()
+      Files.deleteIfExists(file.toPath)
+    };
   }
 
   /**
@@ -320,7 +337,7 @@ abstract class AbstractIndex[K, V](@volatile var file: File, val baseOffset: Lon
    */
   protected[log] def forceUnmap() {
     try MappedByteBuffers.unmap(file.getAbsolutePath, mmap)
-    finally mmap = null // Accessing unmapped mmap crashes JVM by SEGV so we null it out to be safe
+    finally _mmap = null // Accessing unmapped mmap crashes JVM by SEGV so we null it out to be safe
   }
 
   /**
@@ -338,6 +355,22 @@ abstract class AbstractIndex[K, V](@volatile var file: File, val baseOffset: Lon
     }
   }
 
+  /**
+   * Execute the given function in lock and with unmap/map for Windows. This
+   * is necessary because windows locks files, so for any structural operation on
+   * a file (rename, delet, move), it has to be unmappped first, then remapped.
+   */
+  protected def maybeUnmap[T](lock: Lock)(fun: => T): T = {
+    if (OperatingSystem.IS_WINDOWS) {
+      inLock(lock) {
+        safeForceUnmap();
+        fun
+      }
+    } else {
+      fun
+    }
+  }
+
   /**
    * To parse an entry in the index.
    *
diff --git a/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala b/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala
index d8d048410..9c213ce81 100644
--- a/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala
+++ b/core/src/test/scala/unit/kafka/log/OffsetIndexTest.scala
@@ -188,8 +188,8 @@ class OffsetIndexTest {
   def forceUnmapTest(): Unit = {
     val idx = new OffsetIndex(nonExistentTempFile(), baseOffset = 0L, maxIndexSize = 10 * 8)
     idx.forceUnmap()
-    // mmap should be null after unmap causing lookup to throw a NPE
-    intercept[NullPointerException](idx.lookup(1))
+    // mmap should be null after unmap, but mmap() remaps it, so this should not throw any exception
+    idx.lookup(1)
   }
 
   @Test
